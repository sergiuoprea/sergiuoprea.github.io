% Encoding: UTF-8
---
---

@string{aps = {American Physical Society,}}

@article{PhysRev.47.777,
  abbr={PhysRev},
  title={Can Quantum-Mechanical Description of Physical Reality Be Considered Complete?},
  author={Einstein, A. and Podolsky, B. and Rosen, N.},
  abstract={In a complete theory there is an element corresponding to each element of reality. A sufficient condition for the reality of a physical quantity is the possibility of predicting it with certainty, without disturbing the system. In quantum mechanics in the case of two physical quantities described by non-commuting operators, the knowledge of one precludes the knowledge of the other. Then either (1) the description of reality given by the wave function in quantum mechanics is not complete or (2) these two quantities cannot have simultaneous reality. Consideration of the problem of making predictions concerning a system on the basis of measurements made on another system that had previously interacted with it leads to the result that if (1) is false then (2) is also false. One is thus led to conclude that the description of reality as given by a wave function is not complete.},
  journal={Phys. Rev.,},
  volume={47},
  issue={10},
  pages={777--780},
  numpages={0},
  year={1935},
  month={May},
  publisher=aps,
  doi={10.1103/PhysRev.47.777},
  url={http://link.aps.org/doi/10.1103/PhysRev.47.777},
  html={https://journals.aps.org/pr/abstract/10.1103/PhysRev.47.777},
  pdf={example_pdf.pdf},
  selected={true}
}

@Article{MartinezGonzalez2019,
  author    = {Pablo Martinez-Gonzalez and Sergiu Oprea and Alberto Garcia-Garcia and Alvaro Jover-Alvarez and Sergio Orts-Escolano and Jose Garcia-Rodriguez},
  journal   = {Virtual Reality},
  title     = {{UnrealROX}: an extremely photorealistic virtual reality environment for robotics simulations and synthetic data generation},
  year      = {2019},
  month     = aug,
  number    = {2},
  pages     = {271--288},
  volume    = {24},
  abbr      = {Virtual Real.},
  abstract  = {Data-driven algorithms have surpassed traditional techniques in almost every aspect in robotic vision problems. Such algorithms need vast amounts of quality data to be able to work properly after their training process. Gathering and annotating that sheer amount of data in the real world is a time-consuming and error-prone task. These problems limit scale and quality. Synthetic data generation has become increasingly popular since it is faster to generate and automatic to annotate. However, most of the current datasets and environments lack realism, interactions, and details from the real world. UnrealROX is an environment built over Unreal Engine 4 which aims to reduce that reality gap by leveraging hyperrealistic indoor scenes that are explored by robot agents which also interact with objects in a visually realistic manner in that simulated world. Photorealistic scenes and robots are rendered by Unreal Engine into a virtual reality headset which captures gaze so that a human operator can move the robot and use controllers for the robotic hands; scene information is dumped on a per-frame basis so that it can be reproduced offline to generate raw data and ground truth annotations. This virtual reality environment enables robotic vision researchers to generate realistic and visually plausible data with full ground truth for a wide variety of problems such as class and instance semantic segmentation, object detection, depth estimation, visual grasping, and navigation.},
  doi       = {10.1007/s10055-019-00399-5},
  publisher = {Springer Science and Business Media {LLC}},
  selected  = {true},
  url       = {https://doi.org/10.1007/s10055-019-00399-5},
}

@Article{GarciaGarcia2018a,
  author    = {Alberto Garcia-Garcia and Sergio Orts-Escolano and Sergiu Oprea and Victor Villena-Martinez and Pablo Martinez-Gonzalez and Jose Garcia-Rodriguez},
  journal   = {Applied Soft Computing},
  title     = {A survey on deep learning techniques for image and video semantic segmentation},
  year      = {2018},
  month     = sep,
  pages     = {41--65},
  volume    = {70},
  abbr      = {ASOC},
  abstract  = {Image semantic segmentation is more and more being of interest for computer vision and machine learning researchers. Many applications on the rise need accurate and efficient segmentation mechanisms: autonomous driving, indoor navigation, and even virtual or augmented reality systems to name a few. This demand coincides with the rise of deep learning approaches in almost every field or application target related to computer vision, including semantic segmentation or scene understanding. This paper provides a review on deep learning methods for semantic segmentation applied to various application areas. Firstly, we formulate the semantic segmentation problem and define the terminology of this field as well as interesting background concepts. Next, the main datasets and challenges are exposed to help researchers decide which are the ones that best suit their needs and goals. Then, existing methods are reviewed, highlighting their contributions and their significance in the field. We also devote a part of the paper to review common loss functions and error metrics for this problem. Finally, quantitative results are given for the described methods and the datasets in which they were evaluated, following up with a discussion of the results. At last, we point out a set of promising future works and draw our own conclusions about the state of the art of semantic segmentation using deep learning techniques.},
  doi       = {10.1016/j.asoc.2018.05.018},
  publisher = {Elsevier {BV}},
  selected  = {true},
  url       = {https://doi.org/10.1016/j.asoc.2018.05.018},
}

@Article{GomezDonoso2017,
  author    = {Francisco Gomez-Donoso and Sergio Orts-Escolano and Alberto Garcia-Garcia and Jose Garcia-Rodriguez and John Alejandro Castro-Vargas and Sergiu Ovidiu-Oprea and Miguel Cazorla},
  journal   = {Pattern Recognition Letters},
  title     = {A robotic platform for customized and interactive rehabilitation of persons with disabilities},
  year      = {2017},
  month     = nov,
  pages     = {105--113},
  volume    = {99},
  abbr      = {PRL},
  abstract  = {In this work, we have developed a multisensor system for rehabilitation and interaction with persons with motor and cognitive disabilities. The system enables them to perform different therapies using multiple modes of interaction (head and body pose, hand gestures, voice, touch and gaze) depending on the type and degree of disability. Through a training process, the system can be customized enabling the definition of patients' own gestures for each sensor. The system is integrated with a range of applications for rehabilitation. Examples of these applications are puzzle solving, mazes and text writing using predictive text tools. The system also provides a flexible and modular framework for the development of new applications oriented towards novel rehabilitation therapies. The proposed system has been integrated in a mobile robotic platform and uses low-cost sensors allowing to perform non-intrusive rehabilitation therapies at home. Videos showing the proposed system and users interacting in different ways (multimodal) are available on our project website www.rovit.ua.es/patente/.},
  doi       = {10.1016/j.patrec.2017.05.027},
  publisher = {Elsevier {BV}},
  url       = {https://doi.org/10.1016/j.patrec.2017.05.027},
}

@InProceedings{GarciaGarcia2018,
  author    = {Alberto Garcia-Garcia and Pablo Martinez-Gonzalez and Sergiu Oprea and John Alejandro Castro-Vargas and Sergio Orts-Escolano and Jose Garcia-Rodriguez and Alvaro Jover-Alvarez},
  booktitle = {2018 {IEEE}/{RSJ} International Conference on Intelligent Robots and Systems ({IROS})},
  title     = {The {RobotriX}: {A}n {E}xtremely {P}hotorealistic and {V}ery-{L}arge-{S}cale {I}ndoor {D}ataset of {S}equences with {R}obot {T}rajectories and {I}nteractions},
  year      = {2018},
  month     = oct,
  publisher = {{IEEE}},
  abbr      = {IROS},
  abstract  = {Enter the RobotriX, an extremely photorealistic indoor dataset designed to enable the application of deep learning techniques to a wide variety of robotic vision problems. The RobotriX consists of hyperrealistic indoor scenes which are explored by robot agents which also interact with objects in a visually realistic manner in that simulated world. Photorealistic scenes and robots are rendered by Unreal Engine into a virtual reality headset which captures gaze so that a human operator can move the robot and use controllers for the robotic hands; scene information is dumped on a per-frame basis so that it can be reproduced offline using UnrealCV to generate raw data and ground truth labels. By taking this approach, we were able to generate a dataset of 38 semantic classes across 512 sequences totaling 8M stills recorded at +60 frames per second with full HD resolution. For each frame, RGB-D and 3D information is provided with full annotations in both spaces. Thanks to the high quality and quantity of both raw information and annotations, the RobotriX will serve as a new milestone for investigating 2D and 3D robotic vision tasks with large-scale data-driven techniques.},
  doi       = {10.1109/iros.2018.8594495},
  selected  = {true},
  url       = {https://doi.org/10.1109/iros.2018.8594495},
}

@Article{GarciaGarcia2016,
  author    = {Alberto Garcia-Garcia and Sergio Orts-Escolano and Sergiu Oprea and Jose Garcia-Rodriguez and Jorge Azorin-Lopez and Marcelo Saval-Calvo and Miguel Cazorla},
  journal   = {Neural Computing and Applications},
  title     = {{M}ulti-sensor 3{D} object dataset for object recognition with full pose estimation},
  year      = {2016},
  month     = mar,
  number    = {5},
  pages     = {941--952},
  volume    = {28},
  abbr      = {NCA},
  abstract  = {In this work, we propose a new dataset for 3D object recognition using the new high-resolution Kinect V2 sensor and some other popular low-cost devices like PrimeSense Carmine. Since most already existing datasets for 3D object recognition lack some features such as 3D pose information about objects in the scene, per pixel segmentation or level of occlusion, we propose a new one combining all this information in a single dataset that can be used to validate existing and new 3D object recognition algorithms. Moreover, with the advent of the new Kinect V2 sensor we are able to provide high-resolution data for RGB and depth information using a single sensor, whereas other datasets had to combine multiple sensors. In addition, we will also provide semiautomatic segmentation and semantic labels about the different parts of the objects so that the dataset could be used for testing robot grasping and scene labeling systems as well as for object recognition.},
  doi       = {10.1007/s00521-016-2224-9},
  publisher = {Springer Science and Business Media {LLC}},
  url       = {https://doi.org/10.1007/s00521-016-2224-9},
}

@Article{GarciaGarcia2017,
  author    = {Alberto Garcia-Garcia and Jose Garcia-Rodriguez and Sergio Orts-Escolano and Sergiu Oprea and Francisco Gomez-Donoso and Miguel Cazorla},
  journal   = {Computer Vision and Image Understanding},
  title     = {{A} study of the effect of noise and occlusion on the accuracy of convolutional neural networks applied to 3{D} object recognition},
  year      = {2017},
  month     = nov,
  pages     = {124--134},
  volume    = {164},
  abbr      = {CVIU},
  abstract  = {In this work, we carry out a study of the effect of adverse conditions, which characterize real-world scenes, on the accuracy of a Convolutional Neural Network applied to 3D object class recognition. Firstly, we discuss possible ways of representing 3D data to feed the network. In addition, we propose a set of representations to be tested. Those representations consist of a grid-like structure (fixed and adaptive) and a measure for the occupancy of each cell of the grid (binary and normalized point density). After that, we propose and implement a Convolutional Neural Network for 3D object recognition using Caffe. At last, we carry out an in-depth study of the performance of the network over a 3D CAD model dataset, the Princeton ModelNet project, synthetically simulating occlusions and noise models featured by common RGB-D sensors. The results show that the volumetric representations for 3D data play a key role on the recognition process and Convolutional Neural Network can be considerably robust to noise and occlusions if a proper representation is chosen.},
  doi       = {10.1016/j.cviu.2017.06.006},
  publisher = {Elsevier {BV}},
  url       = {https://doi.org/10.1016/j.cviu.2017.06.006},
}

@Article{Oprea2019,
  author    = {Sergiu Oprea and Pablo Martinez-Gonzalez and Alberto Garcia-Garcia and John A. Castro-Vargas and Sergio Orts-Escolano and Jose Garcia-Rodriguez},
  journal   = {Computers {\&} Graphics},
  title     = {A visually realistic grasping system for object manipulation and interaction in virtual reality environments},
  year      = {2019},
  month     = oct,
  pages     = {77--86},
  volume    = {83},
  abstract  = {Interaction in virtual reality (VR) environments (e.g. grasping and manipulating virtual objects) is essential to ensure a pleasant and immersive experience. In this work, we propose a visually realistic, flexible and robust grasping system that enables real-time interactions in virtual environments. Resulting grasps are visually realistic because hand is automatically fitted to the object shape from a position and orientation determined by the user using the VR handheld controllers (e.g. Oculus Touch motion controllers). Our approach is flexible because it can be adapted to different hand meshes (e.g. human or robotic hands) and it is also easily customizable. Moreover, it enables interaction with different objects regardless their geometries. In order to validate our proposal, an exhaustive qualitative and quantitative performance analysis has been carried out. On one hand, qualitative evaluation was used in the assessment of abstract aspects, such as motor control, finger movement realism, and interaction realism. On the other hand, for the quantitative evaluation a novel metric has been proposed to visually analyze the performed grips. Performance analysis results indicate that previous experience with our grasping system is not a prerequisite for an enjoyable, natural and intuitive VR interaction experience.},
  doi       = {10.1016/j.cag.2019.07.003},
  publisher = {Elsevier {BV}},
  selected  = {true},
  url       = {https://doi.org/10.1016/j.cag.2019.07.003},
}

@Article{Oprea2020,
  author    = {Sergiu Oprea and Pablo Martinez-Gonzalez and Alberto Garcia-Garcia and John Alejandro Castro-Vargas and Sergio Orts-Escolano and Jose Garcia-Rodriguez and Antonis Argyros},
  journal   = {{IEEE} Transactions on Pattern Analysis and Machine Intelligence},
  title     = {{A} {R}eview on {D}eep {L}earning {T}echniques for {V}ideo {P}rediction},
  year      = {2020},
  pages     = {1--1},
  abbr      = {TPAMI},
  abstract  = {The ability to predict, anticipate and reason about future outcomes is a key component of intelligent decision-making systems. In light of the success of deep learning in computer vision, deep-learning-based video prediction emerged as a promising research direction. Defined as a self-supervised learning task, video prediction represents a suitable framework for representation learning, as it demonstrated potential capabilities for extracting meaningful representations of the underlying patterns in natural videos. Motivated by the increasing interest in this task, we provide a review on the deep learning methods for prediction in video sequences. We firstly define the video prediction fundamentals, as well as mandatory background concepts and the most used datasets. Next, we carefully analyze existing video prediction models organized according to a proposed taxonomy, highlighting their contributions and their significance in the field. The summary of the datasets and methods is accompanied with experimental results that facilitate the assessment of the state of the art on a quantitative basis. The paper is summarized by drawing some general conclusions, identifying open research challenges and by pointing out future research directions.},
  doi       = {10.1109/tpami.2020.3045007},
  publisher = {Institute of Electrical and Electronics Engineers ({IEEE})},
  selected  = {true},
  url       = {https://doi.org/10.1109/tpami.2020.3045007},
}

@Article{VillenaMartinez2020,
  author    = {Victor Villena-Martinez and Sergiu Oprea and Marcelo Saval-Calvo and Jorge Azorin-Lopez and Andres Fuster-Guillo and Robert B. Fisher},
  journal   = {Applied Sciences},
  title     = {{W}hen {D}eep {L}earning {M}eets {D}ata {A}lignment: {A} {R}eview on {D}eep {R}egistration {N}etworks ({DRN}s)},
  year      = {2020},
  month     = oct,
  number    = {21},
  pages     = {7524},
  volume    = {10},
  abstract  = {This paper reviews recent deep learning-based registration methods. Registration is the process that computes the transformation that aligns datasets, and the accuracy of the result depends on multiple factors. The most significant factors are the size of input data; the presence of noise, outliers and occlusions; the quality of the extracted features; real-time requirements; and the type of transformation, especially those defined by multiple parameters, such as non-rigid deformations. Deep Registration Networks (DRNs) are those architectures trying to solve the alignment task using a learning algorithm. In this review, we classify these methods according to a proposed framework based on the traditional registration pipeline. This pipeline consists of four steps: target selection, feature extraction, feature matching, and transform computation for the alignment. This new paradigm introduces a higher-level understanding of registration, which makes explicit the challenging problems of traditional approaches. The main contribution of this work is to provide a comprehensive starting point to address registration problems from a learning-based perspective and to understand the new range of possibilities.},
  doi       = {10.3390/app10217524},
  publisher = {{MDPI} {AG}},
  url       = {https://doi.org/10.3390/app10217524},
}

@InProceedings{Oprea2017a,
  author    = {Sergiu-Ovidiu Oprea and Pablo Gil and Damian Mira and Beatriz Alacid},
  booktitle = {Proceedings of the 6th International Conference on Pattern Recognition Applications and Methods},
  title     = {{C}andidate {O}il {S}pill {D}etection in {SLAR} {D}ata - {A} {R}ecurrent {N}eural {N}etwork-based {A}pproach},
  year      = {2017},
  publisher = {{SCITEPRESS} - Science and Technology Publications},
  abbr      = {ICPRAM},
  abstract  = {Intentional oil pollution damages marine ecosystems. Therefore, society and governments require maritime surveillance for early oil spill detection. The fast response in the detection process helps to identify the offenders in the vast majority of cases. Nowadays, it is a human operator whom is trained for carrying out oil spill detection. Operators usually use image processing techniques and data analysis from optical, thermal or radar acquired from aerial vehicles or spatial satellites. The current trend is to automate the oil spill detection process so that this can filter candidate oil spill from an aircraft as a decision support system for human operators. In this work, a robust and automated system for candidate oil spill based on Recurrent Neural Network (RNN) is presented. The aim is to provide a faster identification of the candidate oil spills from SLAR scanned sequences. So far, the majority of the research works about oil spill detection are focused on the classification between real oil spills and look-alikes, and they use SAR or optical images but not SLAR. Furthermore, the overall decision is usually taken by an operator in the research works of state-of-art, mainly due to the wide variety of types of look-alikes which cause false positives in the detection process when traditional NN are used. This work provides a RRN-based approach for candidate oil spill detection using SLAR data in contrast with the traditional Multilayer Perceptron Neural Network (MPNN). The system is tested with temporary data acquired from a SLAR sensor mounted on an aircraft. It achieves a success rate in detecting of 97%.},
  doi       = {10.5220/0006187103720377},
  url       = {https://doi.org/10.5220/0006187103720377},
}

@InProceedings{Oprea2017b,
  author    = {S. Oprea and A. Garcia-Garcia and J. Garcia-Rodriguez and S. Orts-Escolano and M. Cazorla},
  booktitle = {2017 International Joint Conference on Neural Networks ({IJCNN})},
  title     = {{A} recurrent neural network based {S}chaeffer gesture recognition system},
  year      = {2017},
  month     = may,
  publisher = {{IEEE}},
  abbr      = {IJCNN},
  abstract  = {Schaeffer language is considered an effective method to help autistic children overcome communicative disorders. Speech and language therapy results in an improvement in communication skills and understanding of language productions. In this work, a Schaeffer language recognition system is presented with the purpose of teaching children with autism disorder the correct way to communicate using gestures in combination with speech reproduction. The purpose is to accelerate the learning process and increase children interest using a technological approach. A Long Short-Term Memory (LSTM) model has been implemented for this purpose reporting a 93.13% classification success rate over a subset of 25 Schaeffer gestures. A comparison with vanilla RNNs and GRU-based models has been also carried out. Pose-based features such as angles and euclidean distances have been extracted from our gesture dataset by processing raw skeletal data from a Kinect v2 sensor.},
  doi       = {10.1109/ijcnn.2017.7965885},
  url       = {https://doi.org/10.1109/ijcnn.2017.7965885},
}

@Article{AbellanAbenza2017,
  author    = {Javier Abellan-Abenza and Alberto Garcia-Garcia and Sergiu Oprea and David Ivorra-Piqueres and Jose Garcia-Rodriguez},
  journal   = {International Journal of Computer Vision and Image Processing},
  title     = {{C}lassifying {B}ehaviours in {V}ideos with {R}ecurrent {N}eural {N}etworks},
  year      = {2017},
  month     = oct,
  number    = {4},
  pages     = {1--15},
  volume    = {7},
  abstract  = {This article describes how the human activity recognition in videos is a very attractive topic among researchers due to vast possible applications. This article considers the analysis of behaviors and activities in videos obtained with low-cost RGB cameras. To do this, a system is developed where a video is input, and produces as output the possible activities happening in the video. This information could be used in many applications such as video surveillance, disabled person assistance, as a home assistant, employee monitoring, etc. The developed system makes use of the successful techniques of Deep Learning. In particular, convolutional neural networks are used to detect features in the video images, meanwhile Recurrent Neural Networks are used to analyze these features and predict the possible activity in the video.},
  doi       = {10.4018/ijcvip.2017100101},
  publisher = {{IGI} Global},
  url       = {https://doi.org/10.4018/ijcvip.2017100101},
}

@InCollection{Vargas2019,
  author    = {John Alejandro Castro Vargas and Alberto Garcia Garcia and Sergiu Oprea and Sergio Orts Escolano and Jose Garcia Rodriguez},
  booktitle = {Rapid Automation},
  publisher = {{IGI} Global},
  title     = {{O}bject {R}ecognition {P}ipeline},
  year      = {2019},
  pages     = {456--468},
  abstract  = {Object grasping in domestic environments using social robots has an enormous potential to help dependent people with a certain degree of disability. In this chapter, the authors make use of the well-known Pepper social robot to carry out such task. They provide an integrated solution using ROS to recognize and grasp simple objects. That system was deployed on an accelerator platform (Jetson TX1) to be able to perform object recognition in real time using RGB-D sensors attached to the robot. By using the system, the authors prove that the Pepper robot shows a great potential for such domestic assistance tasks.},
  doi       = {10.4018/978-1-5225-8060-7.ch021},
  url       = {https://doi.org/10.4018/978-1-5225-8060-7.ch021},
}

@Article{GarciaRodriguez2020,
  author    = {Jose Garcia-Rodriguez and Francisco Gomez-Donoso and Sergiu Oprea and Alberto Garcia-Garcia and Miguel Cazorla and Sergio Orts-Escolano and Zuria Bauer and John Castro-Vargas and Felix Escalona and David Ivorra-Piqueres and Pablo Martinez-Gonzalez and Eugenio Aguirre and Miguel Garcia-Silviente and Marcelo Garcia-Perez and Jose M.-Canas and Francisco Martin-Rico and Jonathan Gines and Francisco Rivas-Montero},
  journal   = {Pattern Recognition Letters},
  title     = {{COMBAHO}: A deep learning system for integrating brain injury patients in society},
  year      = {2020},
  month     = sep,
  pages     = {80--90},
  volume    = {137},
  abbr      = {PRL},
  abstract  = {In the last years, the care of dependent people, either by disease, accident, disability, or age, is one of the current priority research topics in developed countries. Moreover, such care is intended to be at patients home, in order to minimize the cost of therapies. Patients rehabilitation will be fulfilled when their integration in society is achieved, either in the family or in a work environment. To address this challenge, we propose the development and evaluation of an assistant for people with acquired brain injury or dependents. This assistant is twofold: in the patient's home is based on the design and use of an intelligent environment with abilities to monitor and active learning, combined with an autonomous social robot for interactive assistance and stimulation. On the other hand, it is complemented with an outdoor assistant, to help patients under disorientation or complex situations. This involves the integration of several existing technologies and provides solutions to a variety of technological challenges. Deep leaning-based techniques are proposed as core technology to solve these problems.},
  doi       = {10.1016/j.patrec.2019.02.013},
  publisher = {Elsevier {BV}},
  url       = {https://doi.org/10.1016/j.patrec.2019.02.013},
}

@Article{Oprea2017,
  author    = {S. O. Oprea and A. Garcia-Garcia and S. Orts-Escolano and V. Villena-Martinez and J. A. Castro-Vargas},
  journal   = {Expert Systems},
  title     = {{A} long short-term memory based {S}chaeffer gesture recognition system},
  year      = {2017},
  month     = nov,
  number    = {2},
  volume    = {35},
  abbr      = {Expert Syst.},
  abstract  = {In this work, a Schaeffer language recognition system is proposed in order to help autistic children overcome communicative disorders. Using Schaeffer language as a speech and language therapy, improves children communication skills and at the same time the understanding of language productions. Nevertheless, the teaching process of children in performing gestures properly is not straightforward. For this purpose, this system will teach children with autism disorder the correct way to communicate using gestures in combination with speech reproduction. The main purpose is to accelerate the learning process and increase children interest by using a technological approach. Several recurrent neural network-based approaches have been tested, such as vanilla recurrent neural networks, long short-term memory networks, and gated recurrent unit-based models. In order to select the most suitable model, an extensive comparison has been conducted reporting a 93.13% classification success rate over a subset of 25 Schaeffer gestures by using an long short-term memory-based approach. Our dataset consists on pose-based features such as angles and euclidean distances extracted from the raw skeletal data provided by a Kinect v2 sensor.},
  doi       = {10.1111/exsy.12247},
  publisher = {Wiley},
  url       = {https://doi.org/10.1111/exsy.12247},
}

@InCollection{CastroVargas2019,
  author    = {John-Alejandro Castro-Vargas and Alberto Garcia-Garcia and Sergiu Oprea and Pablo Martinez-Gonzalez and Jose Garcia-Rodriguez},
  booktitle = {Advances in Intelligent Systems and Computing},
  publisher = {Springer International Publishing},
  title     = {3{D} {H}and {J}oints {P}osition {E}stimation with {G}raph {C}onvolutional {N}etworks: {A} {G}raph{H}ands {B}aseline},
  year      = {2019},
  month     = nov,
  pages     = {551--562},
  abbr      = {ROBOT},
  abstract  = {State-of-the-art deep learning-based models used to address hand challenges, e.g. 3D hand joint estimation, need a vast amount of annotated data to achieve a good performance. The lack of data is a problem of paramount importance. Consequently, the use of synthetic datasets for training deep learning models is a trend and represents a promising avenue to improve existing approaches. Nevertheless, currently existing synthetic datasets lack of accurate and complete annotations, realism, and also rich hand-object interactions. For this purpose, in our work we present a synthetic dataset featuring rich hand-object interactions in photorealistic scenarios. The applications of our dataset for hand-related challenges are unlimited. To validate our data, we propose an initial approach to 3D hand joint estimation using a graph convolutional network feeded with point cloud data. Another point in favour of our dataset is that interactions are performed using realistic objects extracted from the YCB dataset. This could allow to test trained systems with our synthetic dataset using images/videos manipulating the same objects in real life.},
  doi       = {10.1007/978-3-030-36150-1_45},
  url       = {https://doi.org/10.1007/978-3-030-36150-1_45},
}

@InCollection{CastroVargas2017,
  author    = {John Alejandro Castro-Vargas and Alberto Garcia-Garcia and Sergiu Oprea and Sergio Orts-Escolano and Jose Garcia-Rodriguez},
  booktitle = {{ROBOT} 2017: Third Iberian Robotics Conference},
  publisher = {Springer International Publishing},
  title     = {{D}etecting and {M}anipulating {O}bjects with a {S}ocial {R}obot: {A}n {A}mbient {A}ssisted {L}iving {A}pproach},
  year      = {2017},
  month     = nov,
  pages     = {613--624},
  abbr      = {ROBOT},
  abstract  = {Object grasping in domestic environments using social robots has an enormous potential to help dependant people with certain degree of disability. In this work, we made use of the well-known Pepper social robot to carry out such task. We provide an integrated solution using ROS to recognize and grasp simple objects. That system was deployed on an accelerator platform (Jetson TX1) to be able to perform object recognition in real time using RGB-D sensors attached to the robot. By using our system, we proved that the Pepper robot shows a great potential for such kind of domestic assistance tasks.},
  doi       = {10.1007/978-3-319-70833-1_50},
  url       = {https://doi.org/10.1007/978-3-319-70833-1_50},
}

@Article{Oprea2021,
  author    = {Sergiu Oprea and Giorgos Karvounas and Pablo Martinez-Gonzalez and Nikolaos Kyriazis and Sergio Orts-Escolano and Iason Oikonomidis and Alberto Garcia-Garcia and Aggeliki Tsoli and Jose Garcia-Rodriguez and Antonis Argyros},
  journal   = {CoRR},
  title     = {{H}-{GAN}: the power of {GAN}s in your {H}ands},
  year      = {2021},
  volume    = {abs/2103.15017},
  abbr      = {IJCNN},
  bibsource = {DBLP, http://dblp.uni-trier.de},
  ee        = {http://arxiv.org/abs/2103.15017},
}

@Comment{jabref-meta: databaseType:bibtex;}
