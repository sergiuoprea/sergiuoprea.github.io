<!DOCTYPE html>
<html>

  <head>
    <meta charset="utf-8">
<meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no">
<meta http-equiv="X-UA-Compatible" content="IE=edge">

<title>Sergiu  Oprea</title>
<meta name="description" content="Research Scientist in the CoreAI team at Meta
">

<!-- Open Graph -->

<meta property="og:site_name" content="Research Scientist in the CoreAI team at Meta
" />
<meta property="og:type" content="object" />
<meta property="og:title" content="" />
<meta property="og:url" content="https://github.com/pages/sergiuoprea/" />
<meta property="og:description" content="About" />
<meta property="og:image" content="" />


<!-- Bootstrap & MDB -->
<link href="https://stackpath.bootstrapcdn.com/bootstrap/4.5.2/css/bootstrap.min.css" rel="stylesheet" integrity="sha512-MoRNloxbStBcD8z3M/2BmnT+rg4IsMxPkXaGh2zD6LGNNFE80W3onsAhRcMAMrSoyWL9xD7Ert0men7vR8LUZg==" crossorigin="anonymous">
<link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/mdbootstrap/4.19.1/css/mdb.min.css" integrity="sha512-RO38pBRxYH3SoOprtPTD86JFOclM51/XTIdEPh5j8sj4tp8jmQIx26twG52UaLi//hQldfrh7e51WzP9wuP32Q==" crossorigin="anonymous" />

<!-- Fonts & Icons -->
<link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/5.14.0/css/all.min.css"  integrity="sha512-1PKOgIY59xJ8Co8+NE6FZ+LOAZKjy+KY8iq0G4B3CyeY6wYHN3yt9PW0XpSriVlkMXe40PTKnXrLnZ9+fkDaog==" crossorigin="anonymous">
<link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/academicons/1.9.0/css/academicons.min.css" integrity="sha512-W4yqoT1+8NLkinBLBZko+dFB2ZbHsYLDdr50VElllRcNt2Q4/GSs6u71UHKxB7S6JEMCp5Ve4xjh3eGQl/HRvg==" crossorigin="anonymous">
<link rel="stylesheet" type="text/css" href="https://fonts.googleapis.com/css?family=Roboto:300,400,500,700|Roboto+Slab:100,300,400,500,700|Material+Icons">

<!-- Code Syntax Highlighting -->
<link rel="stylesheet" href="https://gitcdn.link/repo/jwarby/jekyll-pygments-themes/master/github.css" />

<!-- Styles -->

<link rel="icon" href="data:image/svg+xml,<svg xmlns=%22http://www.w3.org/2000/svg%22 viewBox=%220 0 100 100%22><text y=%22.9em%22 font-size=%2290%22>⛵</text></svg>">

<link rel="stylesheet" href="/pages/sergiuoprea/assets/css/main.css">

<link rel="canonical" href="/pages/sergiuoprea/">

<!-- JQuery -->
<!-- jQuery -->
<script src="https://cdnjs.cloudflare.com/ajax/libs/jquery/3.5.1/jquery.min.js" integrity="sha512-bLT0Qm9VnAYZDflyKcBaQ2gg0hSYNQrJ8RilYldYQ1FxQYoCLtUjuuRuZo+fjqhx/qtq/1itJ0C2ejDxltZVFg==" crossorigin="anonymous"></script>


<!-- Theming-->

  <script src="/pages/sergiuoprea/assets/js/theme.js"></script>
  <!-- Load DarkMode JS -->
<script src="/pages/sergiuoprea/assets/js/dark_mode.js"></script>



  <!-- Global site tag (gtag.js) - Google Analytics -->
  <script async src="https://www.googletagmanager.com/gtag/js?id=G-66E9JY4LTP"></script>
  <script>
    window.dataLayer = window.dataLayer || [];
    function gtag() { dataLayer.push(arguments); }
    gtag('js', new Date());

    gtag('config', 'G-66E9JY4LTP');
  </script>




    
<!-- MathJax -->
<script type="text/javascript">
  window.MathJax = {
    tex: {
      tags: 'ams'
    }
  };
</script>
<script defer type="text/javascript" id="MathJax-script" src="https://cdn.jsdelivr.net/npm/mathjax@3.1.2/es5/tex-mml-chtml.js"></script>
<script defer src="https://polyfill.io/v3/polyfill.min.js?features=es6"></script>


  </head>

  <body class="fixed-top-nav sticky-bottom-footer">

    <!-- Header -->

    <header>

    <!-- Nav Bar -->
    <nav id="navbar" class="navbar navbar-light navbar-expand-sm fixed-top">
    <div class="container">
      
        <!-- Social Icons -->
        <div class="navbar-brand social">
          <a href="mailto:%73%6F%70%72%65%61@%64%74%69%63.%75%61.%65%73"><i class="fas fa-envelope"></i></a>

<a href="https://scholar.google.com/citations?user=JlZbbzIAAAAJ&amp;hl=es" target="_blank" title="Google Scholar"><i class="ai ai-google-scholar"></i></a>

<a href="https://www.researchgate.net/profile/Sergiu-Oprea/" target="_blank" title="ResearchGate"><i class="ai ai-researchgate"></i></a>
<a href="https://github.com/sergiuoprea" target="_blank" title="GitHub"><i class="fab fa-github"></i></a>
<a href="https://www.linkedin.com/in/sergiuoprea" target="_blank" title="LinkedIn"><i class="fab fa-linkedin"></i></a>
<a href="https://twitter.com/sergiuop" target="_blank" title="Twitter"><i class="fab fa-twitter"></i></a>








        </div>
      
      <!-- Navbar Toggle -->
      <button class="navbar-toggler collapsed ml-auto" type="button" data-toggle="collapse" data-target="#navbarNav" aria-controls="navbarNav" aria-expanded="false" aria-label="Toggle navigation">
        <span class="sr-only">Toggle navigation</span>
        <span class="icon-bar top-bar"></span>
        <span class="icon-bar middle-bar"></span>
        <span class="icon-bar bottom-bar"></span>
      </button>
      <div class="collapse navbar-collapse text-right" id="navbarNav">
        <ul class="navbar-nav ml-auto flex-nowrap">
          <!-- About -->
          <li class="nav-item active">
            <a class="nav-link" href="/pages/sergiuoprea/">
              About
              
                <span class="sr-only">(current)</span>
              
            </a>
          </li>
          
          <!-- Blog 
          <li class="nav-item ">
            <a class="nav-link" href="/pages/sergiuoprea/blog/">
              blog
              
            </a>
          </li>
           -->
          <!-- Other pages -->
          
          
          
          
          
          
          
          
          
          <li class="nav-item ">
              <a class="nav-link" href="/pages/sergiuoprea/publications/">
                Publications
                
              </a>
          </li>
          
          
          
          <li class="nav-item ">
              <a class="nav-link" href="/pages/sergiuoprea/teaching/">
                Teaching
                
              </a>
          </li>
          
          
          
          
          
          
          
          
          
          
          <!-- CV -->
          <li class="nav-item ">
            <a class="nav-link" target="_blank" href="https://github.com/pages/sergiuoprea/assets/pdf/short_cv.pdf">CV</a>
          </li>
          
            <div class="toggle-container">
              <a id="light-toggle">
                  <i class="fas fa-moon"></i>
                  <i class="fas fa-sun"></i>
              </a>
            </div>
          
        </ul>
      </div>
    </div>
  </nav>

</header>


    <!-- Content -->

    <div class="container mt-5">
      <div class="post">

  <header class="post-header">
    <h1 class="post-title">
     <span class="font-weight-bold">Sergiu</span>  Oprea
    </h1>
     <p class="desc">Passionate about Deep Learning applied to future video frame prediction.</p>
  </header>

  <article>
    
    <div class="profile float-left">
      
        <img class="img-fluid z-depth-1 rounded" src="/pages/sergiuoprea/assets/img/prof_pic.jpg">
      
      
    </div>
    

    <div class="clearfix">
      <p>Hi! I am Sergiu Oprea, a Ph.D. student at University of Alicante under the direction of Prof. <a href="https://scholar.google.es/citations?user=GNTkqaYAAAAJ&amp;hl=es" target="\_blank">Jose Garcia-Rodriguez</a> and <a href="https://scholar.google.es/citations?user=dznX1DMAAAAJ&amp;hl=es" target="\_blank">Sergio Orts-Escolano</a>.</p>

<p>I am working on pushing the boundaries of future video frame prediction using deep-learning based techniques, especially generative adversarial networks.</p>

<p>Please feel free to contact me through <a href="mailto:soprea@dtic.ua.es" target="\_blank"><img class="emoji" title=":email:" alt=":email:" src="https://github.githubassets.com/images/icons/emoji/unicode/1f4e7.png" height="20" width="20"></a> if you have any questions.</p>

    </div>

    
      <div class="news">
  <h2> <i class="fas fa-lightgrey fa-1x fa-bullhorn"></i> Check this out!</h2>
  
    <div class="table-responsive">
      <table class="table table-sm table-borderless">
      
      
        <tr>
          <th scope="row">Dec 9, 2020</th>
          <td>
            
              Our video prediction review (<a href="https://arxiv.org/abs/2004.05214" target="blank">preprint</a>) was accepted for publication in the TPAMI journal <img class="emoji" title=":tada:" alt=":tada:" src="https://github.githubassets.com/images/icons/emoji/unicode/1f389.png" height="20" width="20"> <img class="emoji" title=":tada:" alt=":tada:" src="https://github.githubassets.com/images/icons/emoji/unicode/1f389.png" height="20" width="20">.

            
          </td>
        </tr>
      
      </table>
    </div>
  
</div>

    

    
      <div class="publications">
  <h2>selected publications</h2>
  <ol class="bibliography">
<li>
<div class="row">
  <div class="col-sm-3">
  
    <img class="img-fluid" src="/pages/sergiuoprea/assets/img/hgan.png">
   
  </div> 

  <div id="Oprea2021" class="col-sm-8">
    
      <div class="title">H-GAN: the power of GANs in your Hands</div>
      <div class="author">
        
          
          
          
          
          
          
            
              
                <em>Oprea, Sergiu</em>,
              
            
          
        
          
          
          
          
          
          
            
              
                
                  Karvounas, Giorgos,
                
              
            
          
        
          
          
          
          
          
          
            
              
                
                  Martinez-Gonzalez, Pablo,
                
              
            
          
        
          
          
          
          
          
          
            
              
                
                  Kyriazis, Nikolaos,
                
              
            
          
        
          
          
          
          
          
          
            
              
                
                  Orts-Escolano, Sergio,
                
              
            
          
        
          
          
          
          
          
          
            
              
                
                  Oikonomidis, Iason,
                
              
            
          
        
          
          
          
          
          
          
            
              
                
                  Garcia-Garcia, Alberto,
                
              
            
          
        
          
          
          
          
          
          
            
              
                
                  Tsoli, Aggeliki,
                
              
            
          
        
          
          
          
          
          
          
            
              
                
                  Garcia-Rodriguez, Jose,
                
              
            
          
        
          
          
          
          
          
          
            
              
                
                  and Argyros, Antonis
                
              
            
          
        
      </div>

      <div class="periodical">
      
        <em>In 2021 International Joint Conference on Neural Networks (IJCNN)</em>
      
      
        2021
      
      
      </div>
    

    <div class="links">
    
      <a class="abstract btn btn-sm z-depth-0" role="button">Abstract</a>
    
    
      <a href="http://arxiv.org/abs/2103.15017" class="btn btn-sm z-depth-0" role="button" target="_blank">arXiv</a>
    
    
    
    
    
    
      <a href="https://github.com/sergiuoprea/hgan" class="btn btn-sm z-depth-0" role="button" target="_blank">Code</a>
    
    
    
    
    
      <a href="https://youtu.be/XP2CJC5tONY" class="btn btn-sm z-depth-0" role="button" target="_blank">Demo</a>
    
    </div>

    <!-- Hidden abstract block -->
    
    <div class="abstract hidden">
      <p>We present HandGAN (H-GAN), a cycle-consistent adversarial learning approach implementing multi-scale perceptual discriminators. It is designed to translate synthetic images of hands to the real domain. Synthetic hands provide complete ground-truth annotations, yet they are not representative of the target distribution of real-world data. We strive to provide the perfect blend of a realistic hand appearance with synthetic annotations. Relying on image-to-image translation, we improve the appearance of synthetic hands to approximate the statistical distribution underlying a collection of real images of hands. H-GAN tackles not only cross-domain tone mapping but also structural differences in localized areas such as shading discontinuities. Results are evaluated on a qualitative and quantitative basis improving previous works. Furthermore, we successfully apply the generated images to the hand classification task.</p>
    </div>
    
  </div>
</div>
</li>
<li>
<div class="row">
  <div class="col-sm-3">
  
    <img class="img-fluid" src="/pages/sergiuoprea/assets/img/videoprediction.jpg">
   
  </div> 

  <div id="Oprea2020" class="col-sm-8">
    
      <div class="title">A Review on Deep Learning Techniques for Video Prediction</div>
      <div class="author">
        
          
          
          
          
          
          
            
              
                <em>Oprea, Sergiu</em>,
              
            
          
        
          
          
          
          
          
          
            
              
                
                  Martinez-Gonzalez, Pablo,
                
              
            
          
        
          
          
          
          
          
          
            
              
                
                  Garcia-Garcia, Alberto,
                
              
            
          
        
          
          
          
          
          
          
            
              
                
                  Castro-Vargas, John Alejandro,
                
              
            
          
        
          
          
          
          
          
          
            
              
                
                  Orts-Escolano, Sergio,
                
              
            
          
        
          
          
          
          
          
          
            
              
                
                  Garcia-Rodriguez, Jose,
                
              
            
          
        
          
          
          
          
          
          
            
              
                
                  and Argyros, Antonis
                
              
            
          
        
      </div>

      <div class="periodical">
      
        <em>IEEE Transactions on Pattern Analysis and Machine Intelligence</em>
      
      
        2020
      
      
      </div>
    

    <div class="links">
    
      <a class="abstract btn btn-sm z-depth-0" role="button">Abstract</a>
    
    
      <a href="http://arxiv.org/abs/2004.05214" class="btn btn-sm z-depth-0" role="button" target="_blank">arXiv</a>
    
    
    
    
    
    
    
    
    
    
    </div>

    <!-- Hidden abstract block -->
    
    <div class="abstract hidden">
      <p>The ability to predict, anticipate and reason about future outcomes is a key component of intelligent decision-making systems. In light of the success of deep learning in computer vision, deep-learning-based video prediction emerged as a promising research direction. Defined as a self-supervised learning task, video prediction represents a suitable framework for representation learning, as it demonstrated potential capabilities for extracting meaningful representations of the underlying patterns in natural videos. Motivated by the increasing interest in this task, we provide a review on the deep learning methods for prediction in video sequences. We firstly define the video prediction fundamentals, as well as mandatory background concepts and the most used datasets. Next, we carefully analyze existing video prediction models organized according to a proposed taxonomy, highlighting their contributions and their significance in the field. The summary of the datasets and methods is accompanied with experimental results that facilitate the assessment of the state of the art on a quantitative basis. The paper is summarized by drawing some general conclusions, identifying open research challenges and by pointing out future research directions.</p>
    </div>
    
  </div>
</div>
</li>
<li>
<div class="row">
  <div class="col-sm-3">
  
    <img class="img-fluid" src="/pages/sergiuoprea/assets/img/unrealrox_cropped.gif">
   
  </div> 

  <div id="MartinezGonzalez2019" class="col-sm-8">
    
      <div class="title">UnrealROX: an extremely photorealistic virtual reality environment for robotics simulations and synthetic data generation</div>
      <div class="author">
        
          
          
          
          
          
          
            
              
                
                  Martinez-Gonzalez, Pablo,
                
              
            
          
        
          
          
          
          
          
          
            
              
                <em>Oprea, Sergiu</em>,
              
            
          
        
          
          
          
          
          
          
            
              
                
                  Garcia-Garcia, Alberto,
                
              
            
          
        
          
          
          
          
          
          
            
              
                
                  Jover-Alvarez, Alvaro,
                
              
            
          
        
          
          
          
          
          
          
            
              
                
                  Orts-Escolano, Sergio,
                
              
            
          
        
          
          
          
          
          
          
            
              
                
                  and Garcia-Rodriguez, Jose
                
              
            
          
        
      </div>

      <div class="periodical">
      
        <em>Virtual Reality</em>
      
      
        2019
      
      
      </div>
    

    <div class="links">
    
      <a class="abstract btn btn-sm z-depth-0" role="button">Abstract</a>
    
    
      <a href="http://arxiv.org/abs/1810.06936" class="btn btn-sm z-depth-0" role="button" target="_blank">arXiv</a>
    
    
    
    
    
    
      <a href="https://github.com/3dperceptionlab/unrealrox" class="btn btn-sm z-depth-0" role="button" target="_blank">Code</a>
    
    
    
    
    
    </div>

    <!-- Hidden abstract block -->
    
    <div class="abstract hidden">
      <p>Data-driven algorithms have surpassed traditional techniques in almost every aspect in robotic vision problems. Such algorithms need vast amounts of quality data to be able to work properly after their training process. Gathering and annotating that sheer amount of data in the real world is a time-consuming and error-prone task. These problems limit scale and quality. Synthetic data generation has become increasingly popular since it is faster to generate and automatic to annotate. However, most of the current datasets and environments lack realism, interactions, and details from the real world. UnrealROX is an environment built over Unreal Engine 4 which aims to reduce that reality gap by leveraging hyperrealistic indoor scenes that are explored by robot agents which also interact with objects in a visually realistic manner in that simulated world. Photorealistic scenes and robots are rendered by Unreal Engine into a virtual reality headset which captures gaze so that a human operator can move the robot and use controllers for the robotic hands; scene information is dumped on a per-frame basis so that it can be reproduced offline to generate raw data and ground truth annotations. This virtual reality environment enables robotic vision researchers to generate realistic and visually plausible data with full ground truth for a wide variety of problems such as class and instance semantic segmentation, object detection, depth estimation, visual grasping, and navigation.</p>
    </div>
    
  </div>
</div>
</li>
<li>
<div class="row">
  <div class="col-sm-3">
  
    <img class="img-fluid" src="/pages/sergiuoprea/assets/img/unrealgrasp_cropped.gif">
   
  </div> 

  <div id="Oprea2019" class="col-sm-8">
    
      <div class="title">A visually realistic grasping system for object manipulation and interaction in virtual reality environments</div>
      <div class="author">
        
          
          
          
          
          
          
            
              
                <em>Oprea, Sergiu</em>,
              
            
          
        
          
          
          
          
          
          
            
              
                
                  Martinez-Gonzalez, Pablo,
                
              
            
          
        
          
          
          
          
          
          
            
              
                
                  Garcia-Garcia, Alberto,
                
              
            
          
        
          
          
          
          
          
          
            
              
                
                  Castro-Vargas, John A.,
                
              
            
          
        
          
          
          
          
          
          
            
              
                
                  Orts-Escolano, Sergio,
                
              
            
          
        
          
          
          
          
          
          
            
              
                
                  and Garcia-Rodriguez, Jose
                
              
            
          
        
      </div>

      <div class="periodical">
      
        <em>Computers &amp; Graphics</em>
      
      
        2019
      
      
      </div>
    

    <div class="links">
    
      <a class="abstract btn btn-sm z-depth-0" role="button">Abstract</a>
    
    
      <a href="http://arxiv.org/abs/1903.05238" class="btn btn-sm z-depth-0" role="button" target="_blank">arXiv</a>
    
    
    
    
    
    
      <a href="https://github.com/3dperceptionlab/unrealgrasp" class="btn btn-sm z-depth-0" role="button" target="_blank">Code</a>
    
    
    
    
    
      <a href="https://youtu.be/65gdFdwsTVg" class="btn btn-sm z-depth-0" role="button" target="_blank">Demo</a>
    
    </div>

    <!-- Hidden abstract block -->
    
    <div class="abstract hidden">
      <p>Interaction in virtual reality (VR) environments (e.g. grasping and manipulating virtual objects) is essential to ensure a pleasant and immersive experience. In this work, we propose a visually realistic, flexible and robust grasping system that enables real-time interactions in virtual environments. Resulting grasps are visually realistic because hand is automatically fitted to the object shape from a position and orientation determined by the user using the VR handheld controllers (e.g. Oculus Touch motion controllers). Our approach is flexible because it can be adapted to different hand meshes (e.g. human or robotic hands) and it is also easily customizable. Moreover, it enables interaction with different objects regardless their geometries. In order to validate our proposal, an exhaustive qualitative and quantitative performance analysis has been carried out. On one hand, qualitative evaluation was used in the assessment of abstract aspects, such as motor control, finger movement realism, and interaction realism. On the other hand, for the quantitative evaluation a novel metric has been proposed to visually analyze the performed grips. Performance analysis results indicate that previous experience with our grasping system is not a prerequisite for an enjoyable, natural and intuitive VR interaction experience.</p>
    </div>
    
  </div>
</div>
</li>
<li>
<div class="row">
  <div class="col-sm-3">
  
    <img class="img-fluid" src="/pages/sergiuoprea/assets/img/robotrix_cropped.gif">
   
  </div> 

  <div id="GarciaGarcia2018" class="col-sm-8">
    
      <div class="title">The RobotriX: An Extremely Photorealistic and Very-Large-Scale Indoor Dataset of Sequences with Robot Trajectories and Interactions</div>
      <div class="author">
        
          
          
          
          
          
          
            
              
                
                  Garcia-Garcia, Alberto,
                
              
            
          
        
          
          
          
          
          
          
            
              
                
                  Martinez-Gonzalez, Pablo,
                
              
            
          
        
          
          
          
          
          
          
            
              
                <em>Oprea, Sergiu</em>,
              
            
          
        
          
          
          
          
          
          
            
              
                
                  Castro-Vargas, John Alejandro,
                
              
            
          
        
          
          
          
          
          
          
            
              
                
                  Orts-Escolano, Sergio,
                
              
            
          
        
          
          
          
          
          
          
            
              
                
                  Garcia-Rodriguez, Jose,
                
              
            
          
        
          
          
          
          
          
          
            
              
                
                  and Jover-Alvarez, Alvaro
                
              
            
          
        
      </div>

      <div class="periodical">
      
        <em>In IEEE/RSJ International Conference on Intelligent Robots and Systems (IROS)</em>
      
      
        2018
      
      
        Also at <i>3D Scene Generation workshop</i> (CVPR 2019).
      
      </div>
    

    <div class="links">
    
      <a class="abstract btn btn-sm z-depth-0" role="button">Abstract</a>
    
    
      <a href="http://arxiv.org/abs/1901.06514" class="btn btn-sm z-depth-0" role="button" target="_blank">arXiv</a>
    
    
    
    
    
    
      <a href="https://github.com/3dperceptionlab/therobotrix" class="btn btn-sm z-depth-0" role="button" target="_blank">Code</a>
    
    
    
    
    
      <a href="https://youtu.be/CiRc5tCtCak" class="btn btn-sm z-depth-0" role="button" target="_blank">Demo</a>
    
    </div>

    <!-- Hidden abstract block -->
    
    <div class="abstract hidden">
      <p>Enter the RobotriX, an extremely photorealistic indoor dataset designed to enable the application of deep learning techniques to a wide variety of robotic vision problems. The RobotriX consists of hyperrealistic indoor scenes which are explored by robot agents which also interact with objects in a visually realistic manner in that simulated world. Photorealistic scenes and robots are rendered by Unreal Engine into a virtual reality headset which captures gaze so that a human operator can move the robot and use controllers for the robotic hands; scene information is dumped on a per-frame basis so that it can be reproduced offline using UnrealCV to generate raw data and ground truth labels. By taking this approach, we were able to generate a dataset of 38 semantic classes across 512 sequences totaling 8M stills recorded at +60 frames per second with full HD resolution. For each frame, RGB-D and 3D information is provided with full annotations in both spaces. Thanks to the high quality and quantity of both raw information and annotations, the RobotriX will serve as a new milestone for investigating 2D and 3D robotic vision tasks with large-scale data-driven techniques.</p>
    </div>
    
  </div>
</div>
</li>
</ol>
</div>

    

    <!--
    <div class="social">
      <div class="contact-icons">
        <a href="mailto:%73%6F%70%72%65%61@%64%74%69%63.%75%61.%65%73"><i class="fas fa-envelope"></i></a>

<a href="https://scholar.google.com/citations?user=JlZbbzIAAAAJ&hl=es" target="_blank" title="Google Scholar"><i class="ai ai-google-scholar"></i></a>

<a href="https://www.researchgate.net/profile/Sergiu-Oprea/" target="_blank" title="ResearchGate"><i class="ai ai-researchgate"></i></a>
<a href="https://github.com/sergiuoprea" target="_blank" title="GitHub"><i class="fab fa-github"></i></a>
<a href="https://www.linkedin.com/in/sergiuoprea" target="_blank" title="LinkedIn"><i class="fab fa-linkedin"></i></a>
<a href="https://twitter.com/sergiuop" target="_blank" title="Twitter"><i class="fab fa-twitter"></i></a>








      </div>
      <div class="contact-note">You can even add a little note about which of these is the best way to reach you.
</div>
    </div>
    -->
    <div class="social">
      <div class="contact_note">You can even add a little note about which of these is the best way to reach you.
</div>
    </div>
  </article>

</div>

    </div>

    <!-- Footer -->

    
<footer class="sticky-bottom mt-5">
  <div class="container">
    © Copyright 2024 Sergiu  Oprea.
    Powered by <a href="http://jekyllrb.com/" target="_blank">Jekyll</a> with <a href="https://github.com/alshedivat/al-folio">al-folio</a> theme. Hosted by <a href="https://pages.github.com/" target="_blank">GitHub Pages</a>.

    
    
    Last updated: September 15, 2024.
    
  </div>
</footer>



  </body>

  <!-- Bootsrap & MDB scripts -->
<script src="https://cdnjs.cloudflare.com/ajax/libs/popper.js/2.4.4/umd/popper.min.js" integrity="sha512-eUQ9hGdLjBjY3F41CScH3UX+4JDSI9zXeroz7hJ+RteoCaY+GP/LDoM8AO+Pt+DRFw3nXqsjh9Zsts8hnYv8/A==" crossorigin="anonymous"></script>
<script src="https://stackpath.bootstrapcdn.com/bootstrap/4.5.2/js/bootstrap.min.js" integrity="sha512-M5KW3ztuIICmVIhjSqXe01oV2bpe248gOxqmlcYrEzAvws7Pw3z6BK0iGbrwvdrUQUhi3eXgtxp5I8PDo9YfjQ==" crossorigin="anonymous"></script>
<script src="https://cdnjs.cloudflare.com/ajax/libs/mdbootstrap/4.19.1/js/mdb.min.js" integrity="sha512-Mug9KHKmroQFMLm93zGrjhibM2z2Obg9l6qFG2qKjXEXkMp/VDkI4uju9m4QKPjWSwQ6O2qzZEnJDEeCw0Blcw==" crossorigin="anonymous"></script>

  
<!-- Mansory & imagesLoaded -->
<script defer src="https://unpkg.com/masonry-layout@4/dist/masonry.pkgd.min.js"></script>
<script defer src="https://unpkg.com/imagesloaded@4/imagesloaded.pkgd.min.js"></script>
<script defer src="/pages/sergiuoprea/assets/js/mansory.js" type="text/javascript"></script>


  


<!-- Load Common JS -->
<script src="/pages/sergiuoprea/assets/js/common.js"></script>


</html>
